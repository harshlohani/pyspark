{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4cefa33e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /Users/harsh/anaconda3/lib/python3.11/site-packages (2.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/harsh/anaconda3/lib/python3.11/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/harsh/anaconda3/lib/python3.11/site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/harsh/anaconda3/lib/python3.11/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /Users/harsh/anaconda3/lib/python3.11/site-packages (from pandas) (1.24.3)\n",
      "Requirement already satisfied: six>=1.5 in /Users/harsh/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd0c2948",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "162a03de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Cities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Pallavaram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Karnataka</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Tamil Nadu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Qatar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Ahmedabad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>117</td>\n",
       "      <td>Kochi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>118</td>\n",
       "      <td>Bhubaneswar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>119</td>\n",
       "      <td>Lucknow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>120</td>\n",
       "      <td>Bangalore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>121</td>\n",
       "      <td>Udaipur</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>122 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0         Cities\n",
       "0             0     Pallavaram\n",
       "1             1      Karnataka\n",
       "2             2     Tamil Nadu\n",
       "3             3          Qatar\n",
       "4             4      Ahmedabad\n",
       "..          ...            ...\n",
       "117         117          Kochi\n",
       "118         118    Bhubaneswar\n",
       "119         119        Lucknow\n",
       "120         120     Bangalore \n",
       "121         121        Udaipur\n",
       "\n",
       "[122 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.read_csv(\"unique.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5be17fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "##The SparkSession class is the entry point for working with Spark SQL, which is an API for working with structured data (data organized in rows and columns) in Spark.\n",
    "# By importing SparkSession, you gain access to functionalities for creating SparkSession objects, interacting with data sources, and performing various data processing operations.\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e7d19359",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/05/05 01:22:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/05/05 01:22:39 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#SparkSession.builder: This creates a builder object to configure the SparkSession.\n",
    "#appName('Practice'): This method sets the application name as \"Practice\". This name is simply a label for your Spark application and can be helpful for identification in logs or cluster management.\n",
    "#getOrCreate(): This method builds the SparkSession object based on the configurations and attempts to retrieve an existing SparkSession if one is already running. If no existing session is found, it creates a new one.\n",
    "\n",
    "spark=SparkSession.builder.appName('Practice').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "779d9a55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.1.215:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Practice</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1400199d0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "21d1f088",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark=spark.read.csv('unique.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aa7b74b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_c0: string, _c1: string]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "406a4fbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+\n",
      "| _c0|                 _c1|\n",
      "+----+--------------------+\n",
      "|NULL|              Cities|\n",
      "|   0|          Pallavaram|\n",
      "|   1|           Karnataka|\n",
      "|   2|          Tamil Nadu|\n",
      "|   3|               Qatar|\n",
      "|   4|           Ahmedabad|\n",
      "|   5|             Kolkata|\n",
      "|   6|             Haryana|\n",
      "|   7|           New Delhi|\n",
      "|   8|       Kollam/Quilon|\n",
      "|   9|           Bengaluru|\n",
      "|  10| Hyderabad/Hyderabad|\n",
      "|  11|           Hyderabad|\n",
      "|  12|                Pune|\n",
      "|  13|              Kerala|\n",
      "|  14|         Navi Mumbai|\n",
      "|  15|           New Delhi|\n",
      "|  16|              Mohali|\n",
      "|  17|              Rajkot|\n",
      "|  18|          Chandigarh|\n",
      "+----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ec754260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "|_c0|              Cities|\n",
      "+---+--------------------+\n",
      "|  0|          Pallavaram|\n",
      "|  1|           Karnataka|\n",
      "|  2|          Tamil Nadu|\n",
      "|  3|               Qatar|\n",
      "|  4|           Ahmedabad|\n",
      "|  5|             Kolkata|\n",
      "|  6|             Haryana|\n",
      "|  7|           New Delhi|\n",
      "|  8|       Kollam/Quilon|\n",
      "|  9|           Bengaluru|\n",
      "| 10| Hyderabad/Hyderabad|\n",
      "| 11|           Hyderabad|\n",
      "| 12|                Pune|\n",
      "| 13|              Kerala|\n",
      "| 14|         Navi Mumbai|\n",
      "| 15|           New Delhi|\n",
      "| 16|              Mohali|\n",
      "| 17|              Rajkot|\n",
      "| 18|          Chandigarh|\n",
      "| 19|              Mumbai|\n",
      "+---+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/05 01:41:06 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , Cities\n",
      " Schema: _c0, Cities\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///Users/harsh/Desktop/Pyspark/unique.csv\n"
     ]
    }
   ],
   "source": [
    "spark.read.option('header','true').csv('unique.csv').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cbbe29d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df_pyspark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c49a27fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(_c0=None, _c1='Cities'),\n",
       " Row(_c0='0', _c1='Pallavaram'),\n",
       " Row(_c0='1', _c1=' Karnataka'),\n",
       " Row(_c0='2', _c1=' Tamil Nadu'),\n",
       " Row(_c0='3', _c1='Qatar')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc77045",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52f965f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61466342",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61687b22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
